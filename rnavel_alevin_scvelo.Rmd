---
title: "RNA Velocity with Alevin & scVelo"
author: "The VAI Singularity"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: pygments
    number_sections: true
    css: custom.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)

```

# Introduction

This document attempts to describe analysis of RNA Velocity derived from single cell RNASeq data starting as close to the very beginning as possible. There are several algorithms and software packages for performing this analysis. The approach we take here uses the `alevin` facility of the `salmon` sequence aligner, followed by the RNA velocity computation algorithm implemented by `scVelo`. There are other approaches, perhaps most notably `velocyto`. There are various arguments why one or another algorithm is or is not superior. Frequently such arguments are an example of gilding the lilly since these analyses are simply a first step towards a long program of validation in the laboratory. Given the time and resources involved in that journey of validation, it is good to take the best first step you can. But it is even more important to **take that step**. This document is designed to make that step easier. You can then go on to explore other options for that step as time and interest allow. 

Getting your environment set up will require working from the command line. But once the tools you need are there, the rest of the analysis can be done in an R session, including from within an RMarkdown document (like this one). We will make use of the `reticulate` package to call Python from R. (Alternatively you can do the same thing from Python by embedding R with the rpy2 package. It just so happens that we are happiest working in RStudio with RMarkdown documents as our instruments of reproducible research. But, for example, Python and Jupyter notebooks would be just as fine. What you CAN'T do, it turns out, is use rpy2 from within an R session. If you try to embed an R session in a Python session embedded in an R session, things get weird.) 

**Note** that while shell commands need to be run at the command line, both the R and Python commands below are meant to be run from within an RStudio RMarkdown document (such as the one you are reading now). This allows for seamless intercommunication between the R and Python sessions. You can execute both the R and Python code segments below within R studio (adjusting as needed for your own data files).

The analysis proceeds along the following steps:

1. Installation of pre-requisite software components.
2. Downloading and indexing the genome
3. Aligning your sequencing reads to intronic and exonic regions of the genome
4. Performing the RNA Velocity calculation

Finally, this document leans heavily on the [RNA Velocity With Alevin](https://combine-lab.github.io/alevin-tutorial/2020/alevin-velocity/)
tutorial from the Combine lab. But it fills in some gaps, and demonstrates how to complete this analysis completely within an RMarkdown document without switching to an independent Python session.

# Pre-requisites

This document assumes that you are working in a Linux-ish environment (including macOS). If you are using Windows, most of what follows should be pretty easy to follow but you might need to adapt slightly for your environment (for example, mingw or cygwin). 

**Note**: The following has only been thoroughly tested on Linux Ubuntu 18.04. If you find any of this is not platform independent, please [open an issue](https://github.com/VanAndelInstitute/single_cell_analyses/issues) on github.

If you do not have them already, you will need installations of R and Python.

## Python

You will need an installation of Python (verion 3.x). Maintaining Python, its dependencies, its modules, and their dependencies quickly becomes a mess. The easy solution is use an environment management system (`conda`), allowing you to (1) install everything you need without being root and (2) not have to worry about whatever that root person might have installed which conflicts with your requirements. 

There is one catch. To install `miniconda` which we will use to create our environment, you need python already installed. We will install another version of python from within our `conda` environment, but we need python already there to bootsrap things. Catch-22, I know. On MacO2, python 2.7 should already be installed. On Linux, if it is not already there you can install it using your package manager. For example:

```{bash}
sudo apt install python2.7 python-pip
```

On Windows, Python binaries are available from https://www.python.org/. 

Once you have python, you can install miniconda with ease.

```{bash}
wget https://repo.anaconda.com/miniconda/Miniconda2-latest-Linux-x86_64.sh
chmod 755 Miniconda2-latest-Linux-x86_64.sh
./Miniconda2-latest-Linux-x86_64.sh

```

If you don't have `wget` on your system, you can do the same thing with `curl`:

```{bash, eval=FALSE}
curl https://repo.anaconda.com/miniconda/Miniconda2-latest-Linux-x86_64.sh -o Miniconda2-latest-Linux-x86_64.sh
chmod 755 Miniconda2-latest-Linux-x86_64.sh
./Miniconda2-latest-Linux-x86_64.sh

```

If you don't have curl OR wget, then you have done something wrong with your life. No matter, download [the miniconda installer](https://repo.anaconda.com/miniconda/Miniconda2-latest-Linux-x86_64.sh) with a browser of your choice and run it.

Now we can use miniconda to set up an environment that has everything we need for running scVelo.

```{bash}
# in case we are already in the base environment...
conda deactivate

conda create --name scvelo python=3
conda activate scvelo
conda install -c conda-forge numba pytables louvain
# install latest development branch of scvelo
pip install git+https://github.com/theislab/scvelo
conda deactivate

```


## R

An installation of R >= version 4.0 is required for this analysis. The lowest activation energy path to that is via [RStudio](https://rstudio.com/products/rstudio/download/#download).

Then you will also need a number of R packages. These packages themselves have dependencies that must be installed. Use 
your package management system to do something like this:

```{bash}
sudo -s
apt-get update
apt-get -y install libssl-dev
apt-get -y install libcurl4-openssl-dev
apt-get -y install libxml2-dev

```

### Installation without root access

If you do not have root access, you will need to download the ssl, xml2, and curl libraries and build and 
install them in a local directory. First add lines similar to this to your `~/.bashrc`:

```{bash}
export PATH=$PATH:~/local/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/local/lib
CPPFLAGS=-I$HOME/local/include
LDFLAGS=-L$HOME/local/lib
```

And then you can build the necessary tools and install them locally. For example:

```{bash}
cd ~/
mkdir local
mkdir build
cd build
curl https://www.openssl.org/source/openssl-1.1.1g.tar.gz\
 -o openssl-1.1.1g.tar.gz
tar -zxvf openssl-1.1.1g.tar.gz
cd openssl-1.1.1g
./config --prefix=/home/erikor/local
make
make install
```

(Note that the openssl has a `config` script, whereas most other tools have a `configure` script. Irritating, I know. 
But both take the `--prefix` argument to specify the location of your local installation.) 

If needed, you can even build and R the same way. Although if you need to build R, you will also need to install the 
PCRE2 and zlib libraries in a manner analagous to the above. Then:

```{bash}
curl https://cran.r-project.org/src/base/R-4/R-4.0.0.tar.gz -o \
  R-4.0.0.tar.gz
tar -zxvf R-4.0.0.tar.gz
cd R-4.0.0
./configure --prefix=/home/erikor/local --without-x --without-readline --without-pcre1
make
make install
```

## R packages

Once you have R installed you can install the necessary packages:

```{r}
setRepositories(ind = 1:6)
req <- c("Biostrings",  
         "BSgenome",
         "GenomicFeatures",
         "SummarizedExperiment",
         "eisaR",
         "tximeta",
         "rjson",
         "reticulate",
         "SingleCellExperiment",
         "scater", 
         "ggplot2", 
         "reshape2",
         "org.Hs.eg.db",
         "foreach")
         

ix <- which(!req %in% installed.packages())
if(length(ix))  
  install.packages(req[ix])

```

## Salmon

Salmon can be easily installed using `conda`. We will install it in its own environment, separate from the scvelo environment
we created above, to avoid any dependency collisions.

```{bash}
# in case we are already in an environment
conda deactivate

conda config --add channels conda-forge
conda config --add channels bioconda
conda create -n salmon salmon

```

Now the hard part is over. On to mapping and counting.

# Genome index

The next step is to download the appropriate genome fastas and index them. But before we index the genome, we need to do 
some initial steps to facilitate identifying whether a given read aligns to an intronic region, and exonic region, or 
both. 

First we download the genome files. Here we download the human genome. Download an analagous genome for the organism you 
are working with. 

```{bash}
#adjust as needed to point to where you want the data to be stored
cd /mnt/data
mkdir genomes
mdkir genomes/hs34

ROOT_DIR=/mnt/data/genomes/hs38/
cd $ROOT_DIR

curl ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_34/gencode.v34.annotation.gtf.gz \
  -o gencode.v34.annotation.gtf.gz 
  
curl ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_34/GRCh38.primary_assembly.genome.fa.gz \
  -o GRCh38.primary_assembly.genome.fa.gz

gunzip GRCh38.primary_assembly.genome.fa.gz

```

We then extract genomic coordinates of introns and exons, with a flank length of 90 (you can adjust the flank size 
depending on the lengths of your transcript reads).

```{r}
# adjust as needed to point to where you want the indexes stored
# don't forget the trailing slash
root_dir <- "/mnt/data/genomes/hs34/"

gtf <- paste0(root_dir, "gencode.v34.annotation.gtf.gz")
fasta <- paste0(root_dir, "GRCh38.primary_assembly.genome.fa")
grl <- getFeatureRanges(
  gtf = gtf,
  featureType = c("spliced", "intron"), 
  intronType = "separate", 
  flankLength = 90L, 
  joinOverlappingIntrons = FALSE, 
  verbose = TRUE
)

```

We then extract just these regions to a new fasta file, and intron/exon annotations to a new gtf.

```{r}
genome <- Biostrings::readDNAStringSet(
    fasta
)
names(genome) <- sapply(strsplit(names(genome), " "), .subset, 1)
seqs <- GenomicFeatures::extractTranscriptSeqs(
  x = genome, 
  transcripts = grl
)

Biostrings::writeXStringSet(
    seqs, filepath = paste0(root_dir, "GRCh38.primary_assembly.genome.expanded.fa")
)

eisaR::exportToGtf(
  grl, 
  filepath = paste0(root_dir, "gencode.vH34.annotation.expanded.gtf")
)
```

We need lookup tables to match introns and exons to eachother and then to gene ids

```{r}
write.table(
    metadata(grl)$corrgene, 
    file = paste0(root_dir, "gencode.vH34.annotation.expanded.features.tsv"),
    row.names = FALSE, col.names = TRUE, quote = FALSE, sep = "\t"
)

df <- eisaR::getTx2Gene(
    grl, filepath = paste0(root_dir, "gencode.vH34.annotation.expanded.tx2gene.tsv")
)

```

We then index these sequences using salmon (and using the whole genome as a decoy to avoid 
lower quality mapping to introns or exons when perfect matches elsewhere exist). 

```{bash}
grep ">" ${ROOT_DIR}GRCh38.primary_assembly.genome.fa | cut -d ">" -f 2 | cut -d " " -f 1 > ${ROOT_DIR}GRCh38.primary_assembly.genome.chrnames.txt

salmon index \
-t <(cat ${ROOT_DIR}GRCh38.primary_assembly.genome.expanded.fa ${ROOT_DIR}GRCh38.primary_assembly.genome.fa) \
-i ${ROOT_DIR}gencode.vH34.annotation.expanded.sidx --gencode -p 10 \
-d ${ROOT_DIR}GRCh38.primary_assembly.genome.chrnames.txt

```

And create a linked transcriptome json config file with tximeta

```{r}
tximeta::makeLinkedTxome(
  indexDir = paste0(root_dir, "gencode.vH34.annotation.expanded.sidx"), 
  source = "GENCODE", genome = "GRCh38", 
  organism = "Homo sapiens", release = "H34", 
  fasta = paste0(root_dir, "GRCh38.primary_assembly.genome.expanded.fa"), 
  gtf = paste0(root_dir, "gencode.vH34.annotation.expanded.gtf"), 
  write = TRUE, jsonFile = paste0(root_dir, "genomes/gencode.vH34.annotation.expanded.json")
)

# check
rjson::fromJSON(file = paste0(root_dir, "genomes/gencode.vH34.annotation.expanded.json"))

```

Now the hard part is over. Time to align some reads.

# Aligning and counting

Salmon alevin is designed to work with reads generated from 10X chromium chemistry. If you have 
fastqs from V2 or V3 10X Chromium libraries, you are good to go. If you have fastqs with other 
libraries with fixed length barcodes and UMIs, you are also good to go but will need to specify
the barcode and UMI lengths and positions with the `--barcodeLength`, `--umiLength` and `--end` 
parameters. 

## Working with inDrop libraries

What if you have inDrop libraries? The challenge with inDrop libraries is that Barcode 1 can be 
8, 9, 10, or 11 bases long. Salmon alevin cannot deal with variable length barcodes. So, we have 
developed a tool that will take inDrop fastqs and reformat them as if they were 10X genomics 
fastqs, by trimming and/or padding the barcodes and UMIs. Because of the way the barcodes are 
designed, this actually introduces little or no ambiguity in the barcodes, surprisingly enough.

If you need to convert inDrop Fastqs, download the tool, build, and install.

```{bash}
cd ~/build
curl -L https://github.com/vanandelinstitute/intent/archive/master.tar.gz -o intent.tar.gz
tar -zxvf intent.tar.gz
cd intent-master
make

# if you don't have root access, just copy bin/intent somewhere on your path.
# of even just run it from bin/intent. But you can copy it to /usr/local/bin by:
sudo make install
```

Then you can do something along the lines of:

```{bash}
intent Day100_inDrop_L000_R1_001.fastq.gz \
       Day100_inDrop_L000_R2_001.fastq.gz 

```

## Mapping with alevin

And map with alevin. Note that if you do not specify `--forceCells`, then alevin will 
identify the "good" barcodes automatically using a knee plot. But sometimes that does 
not go as expected. So specify your expected cells with `--forceCells` if possible.

Also, if you are not using 10XGenomics chromium V2 style fastqs, you will need to specify
something else in place of `--chromium`. For example, `--chromiumV3`, or explicitly specify 
`--barcodeLength`, `--umiLength` and `--end` (where `--end` is either "5" or "3", specifying 
which end the UMI is on relative to the barcode).

```{bash}
# specify your Read 1 and Read 2 fastq's below.
# you can specify more than 1 for each if your library is split accross multiple
# lanes/runs/etc.
mkdir day60
cd day60

salmon alevin -l ISR -i ${ROOT_DIR}gencode.vH34.annotation.expanded.sidx \
-1 day_60_R1.fastq.gz \
-2 day_60_R2.fastq.gz \
-o alevin_out -p 6 --tgMap ${ROOT_DIR}gencode.vH34.annotation.expanded.tx2gene.tsv \
--dumpFeatures 
--forceCells 1000 \
--chromium

```

The hard part is over. On to calculating RNA Velocity

# RNA Velocity

We can then import the counts from `alevin` using the txiMeta package in R

```{r}
setwd("day60")

tximeta::loadLinkedTxome(paste0(root_dir, "gencode.vH34.annotation.expanded.json"))
txi <- tximeta::tximeta(coldata = data.frame(
  names = "Day60",
  files = "alevin_out/alevin/quants_mat.gz", 
  stringsAsFactors = FALSE
), type = "alevin")
```

We need to extract both the spliced and unspliced counts. 

```{r}
cg <- read.delim(paste0(root_dir, "gencode.vH34.annotation.expanded.features.tsv"),
                 header = TRUE, as.is = TRUE)

## Rename the 'intron' column 'unspliced' to make assay names compatible with scVelo
colnames(cg)[colnames(cg) == "intron"] <- "unspliced"
txis <- tximeta::splitSE(txi, cg, assayName = "counts")
```

Before we export our data into Python for scVelo, let's add some additional annotations that 
will be useful, including both gene annotation and dimensional reductions.

```{r}
library(sceasy)
library(org.Hs.eg.db)

txis <- as(txis, "SingleCellExperiment")
assays(txis) <- list(
    counts = assay(txis, "spliced"),
    spliced = assay(txis, "spliced"),
    unspliced = assay(txis, "unspliced")
)

# annotate genes, discarding ones without annotation in org.Hs.eg.db
ix <- which(gsub("\\..*", "", rownames(txis)) %in% ls(org.Hs.egENSEMBL2EG))
txis <- txis[ix,]
ens <- gsub("\\..*", "", rownames(txis))
eg <- mget(ens, org.Hs.egENSEMBL2EG)
eg <- sapply(eg, function(x) { x[1]})
sym <- mget(eg, org.Hs.egSYMBOL)
sym <- sapply(sym, function(x) { x[1]})
rowData(txis)$symbol <- sym
rowData(txis)$entrez <- eg
rowData(txis)$ensembl <- ens

txis <- scater::logNormCounts(txis)
txis <- scater::runPCA(txis)
txis <- scater::runTSNE(txis, dimred = "PCA")

# let's save our work.
saveRDS(txis, "txis.rds")
```

We can now extract the pieces we need to create an anndata 
object that can be imported into a Python session via `reticulate`.

```{r}
library(reticulate)

# adjust paths below as needed
# see above for more information on installing miniconda
use_condaenv("~/local/share/miniconda/envs/scvelo", 
             conda = "~/local/share/miniconda/bin", 
             required = TRUE)

loompy <- reticulate::import('loompy')
anndata <- reticulate::import('anndata', convert = FALSE)

X <- Matrix::t(counts(txis))
obs <- as.data.frame(colData(txis))
var <- as.data.frame(rowData(txis))
counts <- Matrix::t(assay(txis, "spliced"))
spliced <- Matrix::t(assay(txis, "spliced"))
unspliced <- Matrix::t(assay(txis, "unspliced"))
logcounts <- Matrix::t(assay(txis, "logcounts"))
X_PCA <- reducedDim(txis, "PCA")
X_TSNE <- reducedDim(txis, "TSNE")

adata <- anndata$AnnData(
        X = X,
        obs = obs,
        var = var,
        obsm = list(X_PCA = X_PCA,
                    X_TSNE = X_TSNE),
        layers = list(spliced = spliced, 
                      unspliced = unspliced,
                      logcounts = logcounts)
)
```

Now we can drop into an Python session, accessing the `adata` object we just 
created, and run scVelo on our dataset. Again, this can be done right within your 
RMarkdown document, as RStudio now supports Python sessions "out of the box".

```{python}
import scvelo as scv
import matplotlib
matplotlib.use('AGG')
scv.settings.set_figure_params('scvelo')

# the anndata object we created above is available here in 
# the 'r' namespace and can be imported by python
scv.utils.show_proportions(r.adata)

scv.pp.filter_genes(r.adata, min_shared_counts = 5)
scv.pp.normalize_per_cell(r.adata, enforce = True)
scv.pp.filter_genes_dispersion(r.adata, n_top_genes = 2000)
scv.pp.log1p(r.adata)

scv.pp.moments(r.adata, n_pcs = 30, n_neighbors = 30)

scv.tl.recover_dynamics(r.adata)
scv.pl.velocity_embedding_stream(r.adata, basis='X_TSNE', layer=['velocity'], color=['leiden'],
                                dpi = 300, use_raw = True, save = 'stream1.png')
```

Voila!

Next we will talk about some optimization including pseudocount inflation to better 
normalize away the effect of library depth, merging multiple samples, and importing back 
into R for further analysis and visualization.